{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sherlock\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "import datasets\n",
    "import plots\n",
    "import transformer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, P, L, size, patch, dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.P, self.L, self.size, self.dim = P, L, size, dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(L, 128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "            nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "            nn.BatchNorm2d(64, momentum=0.9),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, (dim*P)//patch**2, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "            nn.BatchNorm2d((dim*P)//patch**2, momentum=0.5),\n",
    "        )\n",
    "\n",
    "        self.vtrans = transformer.ViT(image_size=size, patch_size=patch, dim=(dim*P), depth=2,\n",
    "                                      heads=8, mlp_dim=12, pool='cls')\n",
    "        \n",
    "        self.upscale = nn.Sequential(\n",
    "            nn.Linear(dim, size ** 2),\n",
    "        )\n",
    "        \n",
    "        self.smooth = nn.Sequential(\n",
    "            nn.Conv2d(P, P, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(P, L, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(m):\n",
    "        if type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_normal_(m.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        abu_est = self.encoder(x)\n",
    "        cls_emb = self.vtrans(abu_est)       \n",
    "        cls_emb = cls_emb.view(1, self.P, -1)\n",
    "        # P表示端元数目\n",
    "        abu_est = self.upscale(cls_emb).view(1, self.P, self.size, self.size)\n",
    "        abu_est = self.smooth(abu_est)\n",
    "        re_result = self.decoder(abu_est)\n",
    "        return abu_est, re_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "AutoEncoder                                             [1, 3, 95, 95]            --\n",
       "├─Sequential: 1-1                                       [1, 24, 95, 95]           --\n",
       "│    └─Conv2d: 2-1                                      [1, 128, 95, 95]          20,096\n",
       "│    └─BatchNorm2d: 2-2                                 [1, 128, 95, 95]          256\n",
       "│    └─Dropout: 2-3                                     [1, 128, 95, 95]          --\n",
       "│    └─LeakyReLU: 2-4                                   [1, 128, 95, 95]          --\n",
       "│    └─Conv2d: 2-5                                      [1, 64, 95, 95]           8,256\n",
       "│    └─BatchNorm2d: 2-6                                 [1, 64, 95, 95]           128\n",
       "│    └─LeakyReLU: 2-7                                   [1, 64, 95, 95]           --\n",
       "│    └─Conv2d: 2-8                                      [1, 24, 95, 95]           1,560\n",
       "│    └─BatchNorm2d: 2-9                                 [1, 24, 95, 95]           48\n",
       "├─ViT: 1-2                                              [1, 600]                  217,800\n",
       "│    └─Sequential: 2-10                                 [1, 361, 600]             --\n",
       "│    │    └─Rearrange: 3-1                              [1, 361, 600]             --\n",
       "│    └─Dropout: 2-11                                    [1, 362, 600]             --\n",
       "│    └─Transformer: 2-12                                [1, 362, 600]             --\n",
       "│    │    └─ModuleList: 3-6                             --                        (recursive)\n",
       "│    │    └─LayerNorm: 3-3                              [1, 361, 600]             1,200\n",
       "│    │    └─ModuleList: 3-6                             --                        (recursive)\n",
       "│    │    └─LayerNorm: 3-5                              [1, 361, 600]             (recursive)\n",
       "│    │    └─ModuleList: 3-6                             --                        (recursive)\n",
       "│    └─Identity: 2-13                                   [1, 600]                  --\n",
       "├─Sequential: 1-3                                       [1, 3, 9025]              --\n",
       "│    └─Linear: 2-14                                     [1, 3, 9025]              1,814,025\n",
       "├─Sequential: 1-4                                       [1, 3, 95, 95]            --\n",
       "│    └─Conv2d: 2-15                                     [1, 3, 95, 95]            84\n",
       "│    └─Softmax: 2-16                                    [1, 3, 95, 95]            --\n",
       "├─Sequential: 1-5                                       [1, 156, 95, 95]          --\n",
       "│    └─Conv2d: 2-17                                     [1, 156, 95, 95]          468\n",
       "│    └─ReLU: 2-18                                       [1, 156, 95, 95]          --\n",
       "=========================================================================================================\n",
       "Total params: 4,982,345\n",
       "Trainable params: 4,982,345\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 279.67\n",
       "=========================================================================================================\n",
       "Input size (MB): 5.63\n",
       "Forward/backward pass size (MB): 62.08\n",
       "Params size (MB): 19.05\n",
       "Estimated Total Size (MB): 86.76\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoEncoder(P = 3, L = 156, size = 95, patch = 5, dim = 200)\n",
    "batch_size = 1\n",
    "summary(model, input_size = (batch_size, 156, 95, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, P, L, size, patch, dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.P, self.L, self.size, self.dim = P, L, size, dim\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Conv2d(L, 128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "        #     nn.BatchNorm2d(128, momentum=0.9),\n",
    "        #     nn.Dropout(0.25),\n",
    "        #     nn.LeakyReLU(),\n",
    "        #     nn.Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "        #     nn.BatchNorm2d(64, momentum=0.9),\n",
    "        #     nn.LeakyReLU(),\n",
    "        #     nn.Conv2d(64, (dim*P)//patch**2, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "        #     nn.BatchNorm2d((dim*P)//patch**2, momentum=0.5),\n",
    "        # )\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(L, 128, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "            nn.BatchNorm2d(128, momentum=0.9),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "            nn.BatchNorm2d(64, momentum=0.9),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, (dim*P)//patch**2, kernel_size=(1, 1), stride=(1, 1), padding=(0, 0)),\n",
    "            nn.BatchNorm2d((dim*P)//patch**2, momentum=0.5),\n",
    "        )\n",
    "\n",
    "        self.vtrans = transformer.ViT(image_size=size, patch_size=patch, dim=(dim*P), depth=2,\n",
    "                                      heads=8, mlp_dim=12, pool='cls')\n",
    "        \n",
    "        self.upscale = nn.Sequential(\n",
    "            nn.Linear(dim, size ** 2),\n",
    "        )\n",
    "        \n",
    "        self.smooth = nn.Sequential(\n",
    "            nn.Conv2d(P, P, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(P, L, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def weights_init(m):\n",
    "        if type(m) == nn.Conv2d:\n",
    "            nn.init.kaiming_normal_(m.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        abu_est = self.encoder(x)\n",
    "        cls_emb = self.vtrans(abu_est)       \n",
    "        cls_emb = cls_emb.view(1, self.P, -1)\n",
    "        # P表示端元数目\n",
    "        abu_est = self.upscale(cls_emb).view(1, self.P, self.size, self.size)\n",
    "        abu_est = self.smooth(abu_est)\n",
    "        re_result = self.decoder(abu_est)\n",
    "        return abu_est, re_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7b9862250acc2583c7ce65db11a636026a53282eb9be05786839bc10e538367"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
